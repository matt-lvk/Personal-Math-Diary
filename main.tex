\documentclass[10pt]{article}

\usepackage[utf8]{inputenc}

\usepackage[left=1.5cm,right=1.5cm,top=3cm,bottom=3cm]{geometry}
\geometry{verbose}


%\usepackage{newtxtext}
\usepackage{enumitem}
\setenumerate[1]{leftmargin=3em}

\usepackage{amssymb}
\usepackage{amsmath}
\usepackage{amsthm}
\usepackage{amsopn}

\usepackage{mathrsfs}
\usepackage{mdframed}
\usepackage{tikz-cd}

\usepackage{esint}

\usepackage{thmtools, thm-restate}
\usepackage{hyperref}

\usepackage{verbatim}
\usepackage{mathtools}
\usepackage{fullpage}
\usepackage{dsfont}

\usepackage{physics}
\usepackage{setspace}
\usepackage{epstopdf}
\usepackage[T1]{fontenc}
\usepackage{graphicx}

\usepackage{scalerel,stackengine}
\usepackage{float}
\usepackage{ulem}

\usepackage{authblk}

\usepackage{soul}

\usepackage[sort&compress]{natbib}



\usepackage{tikz}
\usetikzlibrary{trees}



%%%%%%%%%% Mathmematical Things %%%%%%%%%%

%%%%% Theorem, Cor, Lem, etc. %%%%%

\theoremstyle{remark}
\newtheorem{Remark}{Remark}[section]
\newtheorem{Example}{Example}[section]
\theoremstyle{plain}
\newtheorem{Theorem}{Theorem}[section]
\newtheorem{Lemma}{Lemma}[section]
\newtheorem{Corollary}{Corollary}[section]
\newtheorem{Definition}{Definition}[section]
\newtheorem{Proposition}{Proposition}[section]

\newtheorem{assump}{Assumption}
\newtheorem{assumpB}{Assumption}
\renewcommand\theassump{A\arabic{assump}}
\renewcommand\theassumpB{B\arabic{assumpB}}
\DeclareRobustCommand{\rchi}{{\mathpalette\irchi\relax}}
\newcommand{\irchi}[2]{\raisebox{\depth}{$#1\chi$}} % inner command, used by \rchi


\newtheorem{assumptionex}{Assumption}
\newenvironment{assumption}
{\pushQED{\qed}\renewcommand{\qedsymbol}{$\bullet$}\assumptionex}
{\popQED\endassumptionex}

\setcounter{secnumdepth}{2}
\setcounter{tocdepth}{2}
%\addcontentsline{toc}{section}{Abstract}

%%%%% New operators %%%%%

\DeclareMathOperator{\sign}{sign}
\DeclareMathOperator{\erfc}{erfc}
\DeclareMathOperator{\diverg}{div}
\DeclareMathOperator{\Op}{Op}
\DeclareMathOperator{\Ima}{Im}
\DeclareMathOperator{\RePart}{Re}
\DeclareMathOperator{\diag}{diag}
\DeclareMathOperator{\supp}{supp}
\DeclareMathOperator{\Var}{Var}

\def \qed {\hfill \vrule height6pt width 6pt depth 0pt}
\newcommand{\al}{\alpha}
\newcommand{\R}{\mathbb{R}}
\newcommand{\E}{\mathbb{E}}
\newcommand{\N}{\mathbb{N}}
\newcommand{\Z}{\mathbb{Z}}
\newcommand{\C}{\mathbb{C}}
\newcommand{\Q}{\mathbb{Q}}
\newcommand{\p}{\partial}
\newcommand{\unity}{{1\!\!\!\:\mathrm{l}}}
\newcommand{\rightv}{\overset{\rightharpoonup}}
\newcommand{\harpoon}{\overset{\rightharpoonup}}
\newcommand{\hsnorm}[1]{\norm{#1}_{\text{HS}}}
\newcommand{\opnorm}[1]{\norm{#1}_{\text{op}}}
\newcommand{\trnorm}[1]{\norm{#1}_{\Tr}}

\usepackage{ulem}

\usepackage{natbib} % reference manager
%\usepackage[normalem]{ulem} %to strike the words

%%%%% Displaybreaks %%%%%
\allowdisplaybreaks

%%%%% Equation numbering %%%%%
\numberwithin{equation}{section}


\stackMath
\newcommand\reallywidehat[1]{%
	\savestack{\tmpbox}{\stretchto{%
			\scaleto{%
				\scalerel*[\widthof{\ensuremath{#1}}]{\kern-.6pt\bigwedge\kern-.6pt}%
				{\rule[-\textheight/2]{1ex}{\textheight}}%WIDTH-LIMITED BIG WEDGE
			}{\textheight}% 
		}{0.5ex}}%
	\stackon[1pt]{#1}{\tmpbox}%
}


%\renewcommand{\geqslant}{\geq}
%\renewcommand{\leqslant}{\leq}
\renewcommand{\tilde}{\widetilde}
\renewcommand{\leq}{\leqslant}
\renewcommand{\geq}{\geqslant}
\DeclarePairedDelimiter\ceil{\lceil}{\rceil}

\title{Writing Husimi Measure under Hartree Fock equation}
\date{}
\author{}

\begin{document}
\setcounter{secnumdepth}{5}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%
%	Probability Theory
%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Probability Theory}
\subsection{Some ole' Binomal}
\subsubsection{Expected value and variance of Binomial}
\begin{Theorem}\label{binom_exp_var}
	Let $X \approx Bin(n,p)$, then the expected value and variance is given by
	\begin{enumerate}
		\item $\mathbb{E}(X) = np$,
		\item $\Var(X) = np(1-p)$.
	\end{enumerate}
\end{Theorem}
\subsubsection{Binomal approx to Poisson}
\begin{Theorem}\label{binom_approx_poisosn}
	For $\lambda = np $,
	$$ \lim_{n  \to \infty} \binom{n}{k} p^k {(1 - p)}^{n - k} = \frac {\lambda^k} {k!} e^{-\lambda}$$
\end{Theorem}

First, we need the following lemma
\begin{Lemma}\label{inf_binom_over_power}
	Let $\Gamma(x) = \int_0^\infty t^{x-1} e^{-t} \dd{t}$ be the Gamma function, then the following equation holds
	\[
		\lim_{n \to \infty} \frac{\binom{n}{k}}{n^k} = \frac{1}{k!}.
	\]
\end{Lemma}

\begin{proof}[Proof of Lemma \ref{inf_binom_over_power}]
	Note first that
		\[
			\Gamma(k+1) = k \Gamma(k) = \dots = k!\Gamma(1) = k!.
		\]
	and \textit{Stirling's Formula for Gamma function} states that
		\[
			\Gamma(x+1) = \sqrt{2\pi x} x^x e^{-x} \left(1 + \frac{1}{12x} \cdots \right).
		\]
	Then we have
	\begin{align*}
		\lim_{n \to \infty} \frac{\binom{n}{k}}{n^k} & = \lim_{n \to \infty} \frac{n!}{n^k (n-k)! k!}
		\\
		& =  \lim_{n \to \infty} \frac{\Gamma(n+1)}{n^k \Gamma(n-k+1) \Gamma(k+1)}
		\\
		& =  \frac{1}{\Gamma(k+1)}\lim_{n \to \infty} \sqrt{\frac{n}{n-k}} \left(\frac{n}{e}\right)^n \left(\frac{e}{n-k}\right)^{n-k} \frac{1}{n^k}.
	\end{align*}
	Where we used the first order of Stirling's Formula for Gamma function in the last equality.
	
	Next, we only need to prove the limit equals to $1$:
	\begin{align*}
		\lim_{n \to \infty} \sqrt{\frac{n}{n-k}} \left(\frac{n}{e}\right)^n \left(\frac{e}{n-k}\right)^{n-k} \frac{1}{n^k} & = \lim_{n \to \infty} \sqrt{\frac{n}{n-k}} \frac{1}{e^k} \frac{(1-k/n)^k}{(1-k/n)^n}
		\\
		& = 1, 
	\end{align*}
	since
	\begin{align*}
		\sqrt{\frac{n}{n-k}} & \to 1,\\
		(1-k/n)^k &\to 1,\\
		(1-k/n)^n &\to e^{-k},
	\end{align*}
	as $n \to \infty$.
\end{proof}
Now we may prove the main theorem
\begin{proof}[Proof of Theorem \ref{binom_approx_poisosn}]
	Let $ p = \lambda/n$ and fix $k$, we have
	\begin{align*}
		\lim_{n \to \infty} \frac{\binom{n}{k}}{n^k} & = \lim_{n \to \infty} \binom{n}{k} \left(\frac{\lambda}{n}\right)^k\left(1 - \frac{\gamma}{n}\right)^{n-k}
		\\
		& = \lim_{n \to \infty} \frac{n^k}{k!} \left(\frac{\lambda}{n}\right)^k \left( 1 - \frac{\lambda}{n} \right)^{n-k} \quad \text{by Lemma \ref{inf_binom_over_power}}
		\\
		& = \frac{\lambda^k}{k!} \left[ \lim_{n \to \infty} \left(1 - \frac{\lambda}{n}\right)^n \right] \underbrace{\left[\lim_{n \to \infty} \left(1 - \frac{\lambda}{n}\right)^{-k} \right]}_{=1}
		\\
		& = \frac{\lambda^k}{k!} e^{-\lambda}.
	\end{align*}
\end{proof}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{Markov Chain}
\begin{Theorem}[Gambler's ruin]\label{gambler_ruin}
Let $p$ is the probability of winning $\$1$ and $q = 1 - p$ is the probability of losing $\$1$. Suppose that a person starts with $\$i$ amount of money where $0 < i < N$ and $N \in \N$ is the number he would want to walk away with. The probability of him winning from this starting position is given by the following equation:
	\[ a_i =
		\begin{cases}
			\frac{1-\left(\frac{q}{p}\right)^i}{1 - \left(\frac{q}{p}\right)^N} \quad & \text{where } p \neq q,\\
			\frac{i}{N}  &  \text{where } p = q = \frac{1}{2}.
		\end{cases}
	\]
\end{Theorem}
\begin{proof}
	Note that
		\tikzstyle{level 1}=[level distance=3.5cm, sibling distance=3.5cm]
		\tikzstyle{level 2}=[level distance=3.5cm, sibling distance=2cm]
		\tikzstyle{bag} = [text width=4em, text centered]
		\tikzstyle{end} = [circle, minimum width=3pt,fill, inner sep=0pt]
		\begin{center}
				\begin{tikzpicture}[grow=right, sloped]
				\node[bag] {$i$}
				child {
					node[bag] {$i + 1$}        
					edge from parent 
					node[above] {win $\$1$}
					node[below]  {$p$}
				}
				child {
					node[bag] {$i -1$}        
					edge from parent         
					node[above] {Lose $\$1$}
					node[below]  {$q$}
				};
			\end{tikzpicture}
		\end{center}
	Then, we observe
	\begin{align*}
		a_i  &= p a_{i+1} + q a_{i-1} \\
		\underbrace{(p + q)}_{=1}  a_i &= p a_{i+1} + q a_{i-1} \\
		 a_{i+1} - a_i & = \frac{q}{p} (a_i - a_{i-1}).
	\end{align*}
Denote $b_j := a_i - a_{i-1}$ and note that $b_1 = a_1$ since $a_0 = 0$ is the absorption edge, we deduce by recursion that
	\[
		b_{i + 1} =  \left(\frac{q}{p}\right)^{i+1} a_1.
	\]
Moreover, note that
	\begin{align*}
		a_{i+1}  - a_1 &= \sum_{i}^{k = 1} a_{k+1} - a_k 
		\\
		& = \sum_{k=1}^i \left(\frac{q}{p}\right)^k a_1.
		\\
		\Rightarrow a_{i+1}  &= a_1 \left(1+\sum_{k=1}^i \left(\frac{q}{p}\right)^k  \right)
		\\
		& = a_1 \left(\sum_{k=0}^i \left(\frac{q}{p}\right)^k  \right).
	\end{align*}
If $p\neq q$, by geometric series, we obtain
	\[
		a_{i+1} = a_1 \frac{1- \left(\frac{q}{p}\right)^{i+1} }{1- \left(\frac{q}{p}\right)}.
	\]
If $ p = q = 0.5$, then
	\[
		a_{i+1}  = a_1 (i+1).
	\]
	
Now to solve for $a_1$, observe that since at absoprtion point $a_N = 1$, then letting $i + 1 = N$, we get
	\[ 1 = a_N =
\begin{cases}
	a_1 \frac{1-\left(\frac{q}{p}\right)^N}{1 - \left(\frac{q}{p}\right)} \quad & \text{where } p \neq q,\\
	a_1 N  &  \text{where } p = q = \frac{1}{2},
\end{cases}
\]
which implies
	\[ a_1 =
\begin{cases}
	\frac{1-\left(\frac{q}{p}\right)}{1 - \left(\frac{q}{p}\right)^N} \quad & \text{where } p \neq q,\\
	\frac{1}{N}  &  \text{where } p = q = \frac{1}{2}.
\end{cases}
\]
Substituting this back, we obtained the desired equation.
\end{proof}

\begin{Corollary}
	The expectation of stopping time with respect to Brownian motion, i.e. 
	$
		\tau = \inf \{t \geq 0:\ B_\tau \in \{ - \alpha, \beta\} \},
	$
	for any positive integer $\alpha$ and $\beta$ is given by 
	\[
		\mathbb{E}[\tau] = \alpha \beta.
	\]
\end{Corollary}

\begin{proof}
	From Theorem \eqref{gambler_ruin} we observe that $\Pr(B_\tau = b) = \frac{a}{a+b}$. Then, note that since $1 = \Pr(B_\tau = -\alpha) + \Pr(B_\tau = \beta) $, we have
	\[
		\Pr(B_\tau =  - \alpha) = \frac{b}{a+b}.
	\]
	 Since $(B_t^2 - t)_{t \geq 0}$ is a martingale, we by apply Optimal Stopping Theorem where
	\[
		0 = \mathbb{E}[B_0^2 - 0] = \mathbb{E} [B_\tau^2 - \tau], 
	\]
	which implies
	\[
	\mathbb{E}[\tau] = \E[B_\tau^2 ] =  \alpha^2 	\Pr(B_\tau =  - \alpha) + \beta^2 	\Pr(B_\tau =  \beta) = \alpha \beta.
	\]
\end{proof}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%
%	Financial Mathematis
%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\clearpage
\section{Financial Mathematics}
Suppose 
\begin{equation}\label{bgm}
	\begin{cases}
	\dd{S_t} &= \mu S_t \dd{t} + \sigma S_t dW_t \\ 
	S_t \big|_{t=0} &= S_0.
\end{cases}
\end{equation}

and risk free is
\[
\begin{cases}
	\dd{B_t} &= r B_t \dd{t} \\ 
	B_t \big|_{t=0} &= B_0.
\end{cases}
\]

Note that the solution to \eqref{bgm} is
\begin{align*}
	\dd{\log S_t} & = \frac{1}{S_t} \dd{S_t} - \frac{1}{2} \frac{1}{S^2_t} \left(\dd{S_t}\right)^2
	\\
	& = \frac{1}{S_t} \left(\mu S_t \dd{t} + \sigma S_t dW_t \right) - \frac{1}{2} \frac{1}{S^2_t} \big(\sigma^2 S_t^2  \underbrace{\left(\dd{W_t}\right)^2}_{\dd{t}}  + O(dt)\big) 
	\\
	& = \bigg(\mu - \frac{1}{2} \sigma^2 \bigg) \dd{t} + \sigma \dd{W_t}.
	\\
	\Rightarrow  S_t & = S_0 \exp( (\mu - \frac{1}{2} \sigma^2 ) \dd{t} + \sigma \dd{W_t})
\end{align*}

\subsection{Girsanov Theorem}
\begin{Theorem}
	Denote
	\[
		\eta_t := \exp(-\int_0^t \lambda(u) \dd{W^\mathbb{P}(u)} - \frac{1}{2} \int_0^t \lambda^2(u) \dd{u}), 
	\]
	where $W^\mathbb{P}(t)$ is Wiener process under $\mathbb{P}$ and $\lambda(t) $ is $\mathcal{F}_t$-measurable to be chosen. Then we can have the following Wiener process under arbitrage-free Wiener process $W^\mathbb{Q}(t)$ defined by
	\[
		W^\mathbb{Q}(t) = W^\mathbb{P}(t) + \int_0^t \lambda(u) \dd{u}. 
	\]
	The measure $\mathbb{Q}$ is defined by 
	\[
	\mathbb{Q}[A] = \int_A \eta_t \dd{P},
	\]
	where $A \in \mathcal{F}_t$.
\end{Theorem}

We may also write it as 
	\begin{equation}
		\dd{W^\mathbb{Q}} = \dd{W^\mathbb{P}} + \lambda(t) \dd{t}.
	\end{equation}
\begin{Remark}[\textbf{Risk neutral GBM}]
	Choosing $\lambda(t) = \frac{\mu - r}{\sigma}$, the GBM process under risk-neutral measure can be written as
	\begin{align*}
		\dd{S_t} & = \mu S_t \dd{t} + \sigma S_t \dd{W^\mathbb{P}}
		\\
		& = \mu S_t \dd{t} + \sigma S_t \big[\dd{W^\mathbb{Q}} - \lambda(t) \dd{t} \big]
		\\
		& = r S_t \dd{t} + \sigma S_t \dd{W^\mathbb{Q}}.
	\end{align*}
\end{Remark}

\begin{Remark}[\textbf{Change of Numeraire}]
	Choosing $\lambda(t) = \frac{\mu - r}{\sigma}$, the discounted risk neutral process can be expressed as
	\begin{align*}
		\dd{\left(\frac{S_t}{B_t}\right)} & = \frac{S_t}{B_t} (\mu - r) \dd{t} + \sigma \dd{W^\mathbb{P}}
		\\
		& = \frac{S_t}{B_t} (\mu - r) \dd{t} + \sigma \big[\dd{W^\mathbb{Q}} - \lambda(t) \dd{t} \big]
		\\
		& = \frac{S_t}{B_t} \sigma \dd{W^\mathbb{Q}},
	\end{align*}
	which is a Martingale.
\end{Remark}


\begin{Remark}
	Since choice of $\lambda(t)$ is unique here to generate a Martingle process, $\mathbb{Q}$ is known as unique equivalent martingale (EMM) which equals to say that $\mathbb{Q}$ is a risk-neutral measure.
\end{Remark}


\subsection{Black-Scholes}
Suppose the call-option value process (admissible, BS is closed and whole other assumptinos)
\[
	C(S_t) = V_t((S_T-K)^+).
\]
From Ito lemma, we have
\begin{align*}
	\dd{C(S_t, t)} & = \pdv{C(S_t)}{t}\dd{t} +  \pdv{C(S_t)}{S_t}\dd{S_t} + \frac{1}{2} \pdv[2]{C(S_t)}{S_t}  \left(\dd{S_t}\right)^2
	\\
	& = \pdv{C(S_t)}{t}\dd{t}  +  \pdv{C(S_t)}{S_t}\left(r S_t \dd{t} + \sigma S_t dW_t \right) + \frac{1}{2} \pdv[2]{C(S_t)}{S_t}  \sigma^2 S_t^2  \dd{t}
	\\
	&
	= \left(\pdv{C}{t}  + r S_t \pdv{C}{S_t} + \frac{1}{2}  \sigma^2 S_t^2 \pdv[2]{C}{S_t}\right) \dd{t}   + \sigma S_t \pdv{C(S_t)}{S_t} \dd{W_t} .
\end{align*}
Moreover, when we consider risk-free bond, we get
\begin{align*}
	\dd{\left(\frac{C(S_t,t)}{B_t}\right)} & = C(S_t,t) \dd{\left(\frac{1}{B_t}\right)} + \frac{1}{B_t} \dd{C(S_t,t)}
	\\
	& =  -  C(S_t,t) \frac{1}{B^2_t} \dd{B_t} \frac{1}{B_t}  + \frac{1}{B_t} \dd{C(S_t,t)} 
	\\
	& = -  C(S_t,t) \frac{r}{B_t} \dd{t} + \frac{1}{B_t} \dd{C(S_t,t)} 
	\\
	& = \frac{1}{B_t} \left(\pdv{C}{t}  + r S_t \pdv{C}{S_t} + \frac{1}{2}  \sigma^2 S_t^2 \pdv[2]{C}{S_t} - rC\right) \dd{t} + \sigma \frac{S_t}{B_t}\pdv{C(S_t)}{S_t}  \dd{W_t}  
\end{align*}


For no arbitrage, there is no drift term, so the Black-Scholes equation equals to
\begin{equation}\label{black_scholes}
	\pdv{C}{t}  + r S_t \pdv{C}{S_t} + \frac{1}{2}  \sigma^2 S_t^2 \pdv[2]{C}{S_t} -rC = 0
\end{equation}

\begin{Remark}
	If there is a dividend $d$, the BS equation is 
	\[
		\pdv{C}{t}  + ({\color{red}r-d})S_t \pdv{C}{S_t} + \frac{1}{2}  \sigma^2 S_t^2 \pdv[2]{C}{S_t} -rC = 0
	\]
\end{Remark}

\subsubsection{Solving Black-Scholes Equation}
From \eqref{black_scholes}, we denote the operator
	\[
		L := - \left(r S_t \pdv{S_t} + \frac{\sigma^2}{2} S^2_t \pdv[2]{S_t} - r\right)
	\]
Then, BS can be rewritten as
\begin{equation}
	\begin{cases}
		\big(\p_t + L\big) C(S_t, t) &= 0 \\
		C(x, 0) & = (e^x - e^{x_0})^+,
	\end{cases}
\end{equation}
where $x = \log S$.
We can calculate the heat-kernel of the BS-equation by using Fourier Transform, $U_{\textbf{BS}}(\tau; x - y)$. Thus, 
\[
C_{\textbf{BS}}(\tau, x) = \int_{x_0}^\R U_{\textbf{BS}}(\tau; x - y)C(y, 0) \dd{y}
\]


\subsubsection{Derivation of European Call Option}

\begin{Proposition}\label{BS_0}
	Let $g(0,\infty) \to \R_+$ be a claim such that $g(x) \leq c(1+x)$ for some $c\geq 0$ and $X = g(S_T)$. Then the value process $(V_t(X))_{t\in[0,T]}$ satisfies $V_t(X) = \nu(t,S_t)$ where
	\begin{equation}\label{option_v}
		\nu(t,x) = e^{-r(T-t)} \int_\R g(e^y) \phi_{\log(x)+ (T-t)(r-\sigma^2/2),\ \sigma^2(T-t)} (y) \dd{y},
	\end{equation}
	and $\varphi(\mu, \sigma^2)$ is the probability density function of normal distribution i.e.
	\[
	\phi(\mu, \sigma^2)(y) := \frac{1}{\sqrt{2\pi \sigma^2}} \exp(-\frac{(\mu -y)^2}{2\sigma^2}).
	\]
\end{Proposition}

\begin{proof}
	From discounted process we have
	\begin{align*}
		\hat{V}_t(X) &= \E^Q [\hat{X}|\mathcal{F}_t] 
		\\
		& = e^{-rT}  \E^Q [g(S_T)|\mathcal{F}_t] 
		\\
		& =  e^{-rT}  \E^Q \left[g \left( S_t \exp(\left(r - \frac{\sigma^2}{2}\right)(T-t) + \sigma(W_T - W_t))  \right)\mid \mathcal{F}_t\right]
		\\
		& = e^{-rT}  \E^Q \left[g \left( \exp(\log(S_t) + \left(r - \frac{\sigma^2}{2}\right)(T-t) + \sigma(W_T - W_t))  \right)\mid \mathcal{F}_t\right], 
	\end{align*}
	where we used the solution to the Stochastic GBM. Note here that $(W_T - W_t) \sim \mathcal{N}(0, T-t)$. Thus, we have 
	\begin{align*}
		\hat{V}_t(X) &= \E^Q [\hat{X}|\mathcal{F}_t] 
		\\
		& = e^{-rT}  \int_\R g \left( \exp(\log(S_t) + \left(r - \frac{\sigma^2}{2}\right)(T-t) + \sigma {\color{red} y})  \right) \phi_{0, T-t} (y) \dd{y}
		\\
		& =  e^{-rT}  \int_\R g(e^z)  \phi_{\log(x)+ (T-t)(r-\sigma^2/2),\ \sigma^2(T-t)} (z) \dd{z}.
	\end{align*}
	Finally, observe that the discounted value process is given by $\hat{V}_t(X) = e^{-rt} V_t(X)$ then we are done.
\end{proof}

\begin{Theorem}[Call option pricing]\label{BS_Call}
	Let $C(S_t, t) := V_t((S_T-K)^+)$ value process of the call option, then we have
	\begin{equation}
		C(S_t, t) = S_t \Phi(d_1) - K e^{-r(T-t)} \Phi(d_2),
	\end{equation}
	where $\Phi(x)\ (\mathbb{P} (X < x) )$ is the distribution of standard normal distribution $\mathcal{N}(0,1)$ and
	\begin{align*}
		d_1 & := \frac{1}{\sigma \sqrt{T-t}} \left(\log(\frac{S_t}{K}) + (r+\sigma^2/2)(T-t) \right),\\
		d_2 & := d_1 - \sigma \sqrt{T-t}.
	\end{align*}
\end{Theorem}

\begin{proof}
	Let $g(x) = (x-K)^+$ then from Proposition \ref{BS_0}, we get
	\begin{align*}
	C(S_t,t) & = V_t((S_t-K)^+) \\
	& = e^{-r(T-t)} \int_\R \underbrace{(e^y-K)^+}_{=g(e^y)}  \phi_{\log(S_t)+ (T-t)(r-\sigma^2/2),\ \sigma^2(T-t)} (y) \dd{y}
	\\
	& = e^{-r(T-t)} \int_\R \left(x \exp(\left(r-\frac{\sigma}{2}\right)(T-t) + \sigma\sqrt{T-t}{\color{red}z}) - K\right)^+ \phi_{0,1}(z) \dd{z},
	\end{align*}
	where $x = S_t$.
	
	Let
	\begin{align*}
		\tau & := T-t,\\
		\tilde{r} &:= r-\frac{\sigma^2}{2},\\
		\ell &:= \frac{\log(K/x) - \tilde{r}\tau}{\sigma \sqrt{\tau}}.
	\end{align*}
	Thus, we observe that $\left(x \exp(\left(r-\frac{\sigma}{2}\right)(T-t) + \sigma\sqrt{T-t}{\color{red}z}) - K\right)^+$ is non-zero iff
	\begin{align*}
		x \exp(\left(r-\frac{\sigma}{2}\right)(T-t) + \sigma\sqrt{T-t}{\color{red}z}) - K  & > 0 \\
		\Leftrightarrow &\ y > \ell
	\end{align*}
	Then
	\begin{align*}
		C(S_t,t) & = e^{-r\tau} \int_\ell^\infty \left(x \exp(\tilde{r}\tau + \sigma\sqrt{\tau}{\color{red}z}) - K\right) \phi_{0,1}(z) \dd{z} 
		\\
		& = e^{-r\tau} \int_\ell^\infty \left(x \exp(\tilde{r}\tau + \sigma\sqrt{\tau}{\color{red}z}) - K\right) \left(\frac{1}{\sqrt{2\pi}} e^{-\frac{z^2}{2}}\right) \dd{z} 
		\\
		& = x \int_\ell^\infty \frac{1}{\sqrt{2\pi}} \exp(-\frac{(z-\sigma\sqrt{\tau})}{2}) \dd{z} - e^{-r\tau} K \Phi(-\ell)
		\\
		& = x \int^\infty_{\ell - \sigma\sqrt{\tau}} \frac{1}{\sqrt{2\pi}} e^{-\frac{z^2}{2}} \dd{z}- e^{-r\tau} K \Phi(-\ell)
		\\
		& = x \Phi(-\ell + \sigma\sqrt{\tau}) - e^{-r\tau} K \Phi(-\ell)
	\end{align*}
\end{proof}

\begin{Theorem}[Replicating strategies]
	The replicating trading strategy $\varphi = (\varphi^0_t, \varphi^1_t)_{t\in[0,T]}$ of $X$ is given by 
	\begin{equation}
		\begin{aligned}
			\varphi^0_t &= e^{-rt} \left(\nu(S_t,t) - S_t \pdv{x}\nu(S_t,t) \right),\\
			\varphi^1_t &= \pdv{x} \nu(S_t,t),
		\end{aligned}
	\end{equation}
	where $\nu$ is defined in \eqref{option_v}
\end{Theorem}

\subsubsection{Put-Call Parity}
\begin{equation}
	S_t - Ke^{-r(T-t)} = C(S_t, t) - P(S_t,t).
\end{equation}


\subsubsection{Barrier options}
Suppose a down-and-out option
\[
X : = (S_t - K)^+ \mathds{1}_{\min_{t\in[0,T]}S_t > \beta }
\]
The value process is therefore,
\[
 V_t(X) = e^{-r(T-t)} \E^Q [X|\mathcal{F}_t]
\]

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%
%	Time Series
%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\clearpage
\section{Time Series}


\subsection{Stationary and Autocorrelation}

\begin{Definition}
	The \textbf{autocovariance} $$\gamma (s,t) := \mathds{E} \left[(x_s - \mu_s) (x_t-\mu_t)\right],$$ and \textbf{cross-covar.} $$\rho_{x,y}(s,t):= \mathds{E}\left[(x_s - \mu_{xs})(y_t - \mu_{ts})\right].$$
\end{Definition}

\begin{Definition}
	The \textbf{autocorrelation function} (ACF) is defined as
	\[
	\rho_{x} (s,t)  := \frac{\gamma(s,t)}{\sqrt{\gamma(s,s)\gamma(t,t)}},
	\]
\end{Definition}

\textbf{Notation}: $\rho_{x}(t, t+h)  \equiv \rho(h)$.

\begin{Definition}[Strictly stationary]
	For all $k \in \N$, $t_k \in \N$ and $c_k \in \R$. Then the time series is strictly stationary if
	\[
	\mathds{P} \big(x_{t_1} \leq c_1, \dots x_{t_k} \leq c_k \big) = \mathds{P} \big(x_{t_1+h} \leq c_1, \dots x_{t_k+h} \leq c_k \big),
	\]
	for given probability measure $\mathds{P}$ and constant $h \in \N$.
\end{Definition}


\begin{Definition}[Weakly stationary]
	The time series is strictly stationary if
	\begin{enumerate}
		\item $\mu_t = \mathds{E}[x_t]$ is constant, and
		
		\item $\gamma(s,t)$ depends only on $|s-t|$. 
	\end{enumerate}
\end{Definition}


\begin{Lemma}
	Assume time series is weakly stationary. Using the notation $\gamma (t, t+h) \equiv \gamma (h)$, we have
	\begin{enumerate}
		\item $|\gamma (t)| \leq \gamma(0)$,
		
		\item $\gamma(h) = \gamma(-h)$.
	\end{enumerate}
\end{Lemma}

\begin{proof}
	\begin{enumerate}
		\item By Cauchy-Schwarz inequality $|\gamma (t, t+h)|^2 \leq \gamma(t,t) \gamma(t+h,t+h)$ by definition $|\gamma(h)|^2 \leq \gamma(0) \gamma(0)$.
		
		\item From definition of covariance we have
		\begin{align*} 
			\gamma(h) &= \gamma(t+h-t)\\
			& = \mathds{E}\left[(x_{t+h}-\mu)(x_t - \mu)\right]\\
			& = \mathds{E}\left[(x_t - \mu)(x_{t+h}-\mu)\right]\\
			& = \gamma(t-(t+h)) = \gamma(-h)
		\end{align*}
	\end{enumerate}
\end{proof}

\begin{Example}
	Suppose $w_t$ for all $t\in N$ is white noise process, i.e. iid $w_t \approx N(0, \sigma_w^2)$. Given two series
	\begin{align*}
		x_t &= w_t + w_{t-1},\\
		y_t &= w_t - w_{t-1}.
	\end{align*}
	Then for any $h\in \N$, we get the following for (cross-)covariance:
	\begin{enumerate}
		\item \begin{align*}
			\gamma_x(0) &= \mathds{E}[(w_t + w_{t-1})^2] =  \mathds{E}[w_t^2] + \mathds{E}[w_{t-1}^2] + 2\underbrace{ \mathds{E}[w_t w_{t-1}]}_{=0}\\
			& = 2\sigma_w^2,
			\\
			\gamma_y(0) &= \mathds{E}[(w_t - w_{t-1})^2] =  \mathds{E}[w_t^2] + \mathds{E}[w_{t-1}^2] - 2\underbrace{ \mathds{E}[w_t w_{t-1}]}_{=0}\\
			& = 2\sigma_w^2.
		\end{align*}
		\item 
		\begin{align*}
			\gamma_x (1) &= \mathds{E}\left[(w_t + w_{t-1}) (w_{t+1} + w_t) \right] \\
			& = \mathds{E}[w_tw_{t+1}] + \mathds{E}[w_t^2] + \mathds{E}[w_{t-1} w_{t+1}] + \mathds{E}[w_{t-1}w_t]\\
			& = \sigma_w = \gamma_x(-1),
			\\
			\gamma_y (1) &=  \mathds{E}[w_tw_{t+1}] - \mathds{E}[w_t^2] + \mathds{E}[w_{t-1} w_{t+1}] - \mathds{E}[w_{t-1}w_t]\\
			&  = - \sigma_w = \gamma_y(-1).
		\end{align*}
		
		\item $\gamma_{xy}(0) =  \mathds{E}[w_t^2]  -  \mathds{E}[w_{t-1}^2]  = 0$ and $\gamma_{xy} (1) = cov (x_{t+1}, y_t) = -\sigma_w^2$ and $\gamma_{xy} (-1) = cov (x_{t}, y_{t-1}) = -\sigma_w^2$
		
		\item ACF
		\begin{equation}
			\rho_{xy} (h) = 
			\begin{cases}
				0 & h=0,\\
				1/2 & h=1,\\
				-1/2 & h=-1,\\
				0 & |h| \geq 2
			\end{cases}
		\end{equation}
		Thus the joint time series is stationary.
	\end{enumerate}
\end{Example}

\begin{Definition}[Linear Process]
	$\{x_t\}$ is linear process if 
	\[
	x_t = \mu + \sum_{j=-\infty}^\infty \psi_j w_{t-j}, \quad \sum_{j=-\infty}^\infty |\psi_j|< \infty,
	\]
	where $w_t$ is white noise.
\end{Definition}

\begin{Definition}[Gaussian Process]
	$\{x_t\}$ is Gaussian process if the vector $x := (x_{t_1}, x_{t_2},\dots, x_{t_n})' \in \R^n$ has a multivariate normal distribution with density function
	\[
	f(x) = \frac{1}{(2\pi)^{n/2}} \det\big(\Gamma\big)^{-1/2} \exp \left(-\frac{1}{2} (x-\mu)' \Gamma^{-1} (x-\mu) \right),
	\]
	where $\mu \in \R^n$ and $\Gamma:= var (x) = \left\{ \gamma(t_i, t_j);\ i,j=1,\dots,n\right\}$.
\end{Definition}

%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%			ARIMA
%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsubsection{ARIMA}
$ARMA(p,q)$: 
\[
x_t - \phi_1 x_{t-1} - \dots - \phi_p x_{t-p} = w_t + \theta_t w_{t-1} + \dots + \theta_q w_{t-q}.
\]

\subsection{Autoregressive Model}

\begin{Definition}[$AR(p)$]
	Let $w_t$ be white noise, $AR(p)$ is defined as 
	\[
	x_t - \phi_1 x_{t-1} - \cdots - \phi_p x_{t-p} = w_t.
	\]
\end{Definition}

\begin{Definition}[Autoregressive Operator]
	Let $B$ be time-lagged operator, i.e. $Bx_t = x_{t-1}$, the autoregressive operator is defined as
	\[
	\phi(B) = 1-\phi_1 B - \phi_2 B^2 - \cdots - \phi_p B^p.
	\]
\end{Definition}

\begin{Example}
	Assuming  $\phi_j = \phi$ for all $j \in \N$, from AR(1)
	\begin{align*}
		x_t & = \phi x_{t-1} + w_t = \phi(\phi x_{t-2} + w_{t-1}) + w_t\\
		& =  \phi^2 x_{t-2}  + \phi w_{t-1} + w_t\\
		& \vdots\\
		& = \phi^k x_{t-k} + \sum_{j=0}^{k-1} \phi^j w_{t-j}.
	\end{align*}
	Thus, if $|\phi| < 1$ and $x_t$ is sationary then for large $k\to\infty$, we have
	\[
	x_t = \sum_{j=0}^\infty \phi^j w_{t-j}.
	\]
	Taking expectation value,
	\[
	\mathds{E}[x_t] = \sum_{j=0}^\infty	 \phi_j \mathds{E}[w_t] = 0.
	\]
	Moreover, for $h\geq $ the autocovriance function is
	\begin{align*}
		\gamma(h) &= \mathds{E} \left[\left(\sum_{j=0}^\infty \phi^j w_{t+h-j} \right) \left(\sum_{j=0}^\infty \phi^k w_{t-k} \right)\ \right]\\
		& = \mathds{E} \big[(w_{t+h-1}+ \phi w_{t+h-2} + \cdots \phi^h w_{t} + \phi^{h+1} w_{t-1} + \cdots ) (w_t + \phi w_{t-1}+ \cdots)\big]\\
		& = \sigma_w^2 \sum_{j=0} \phi^{h+j} \phi^j = \sigma_w^2 \phi^h \sum_{j=0} \phi^{2j}\\
		& = \frac{\sigma_w^2 \phi^h}{1-\phi^{2}}.
	\end{align*}
	And ACF is
	\[
	\rho(h) = \frac{\gamma(h)}{\gamma(0)} = \phi^h.
	\]
	As well as
	\[
	\rho(h)  =\phi \rho(h-1).
	\]
\end{Example}

\begin{Definition}
	The \textbf{moving average operator} is defined as
	\[
	\theta(B) : = 1 + \theta_1 B + \theta_2 B^2 + \cdots + \theta_q B^q.
	\]
\end{Definition}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%
%	Operational Research
%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\clearpage
\section{Operational Research}


\subsection{Safety Stock}
\begin{Theorem}
	Let the lead-time $T$ be normally distributed, i.e. $T\sim N(\mu_\ell, \sigma^2_\ell)$ and denote the set of demand of period $i$ as $\{D_i\}_{i=1}^T$ where $D_i \sim N(\mu_d, \sigma^2_d)$ for each $1\leq i \leq T$. Then the safety stock level, $SS$ satisifies the following equation
	\[
	SS = z_\alpha \sqrt{\sigma_d^2 \mu_\ell + \mu_d^2 \sigma^2_\ell},
	\]
	where $z_\alpha$ is the Z-value of a desired $\alpha$ which is chosen.
\end{Theorem}

\begin{proof}
	Denote the demand between within lead-time as $D(T) := \sum_{i=1}^T D_i$. The safety stock level is then set at
	\[
	SS = z_\alpha \sqrt{Var(D(T))}.
	\]
	Thus we need only to prove $Var(D(T))$. 
	
	From Towering-property, note that 
	\begin{align*}
		\mathbb{E}[D(T)] & = \mathbb{E} \big[\mathbb{E}[D(T)|T]\big]\\
		&
		= \mathbb{E}\big[T \mu_d\big] = \mu_d \mathbb{E}[T] = \mu_d \mu_\ell.
	\end{align*}
	Furthemore, again with Towering-property, we have
	\begin{align*}
		\mathbb{E}[D^2(T)] & = \mathbb{E} \big[\mathbb{E}[D^2(T)|T]\big]
		\\
		& = \mathbb{E}\big[Var[D(T)|T] + \big(\mathbb{E}[D(T)|T]\big)^2\big]
		\\			
		& = \mathbb{E}\big[T \sigma_d^2 + \big(T \mu_d \big)^2\big]
		\\
		& = \sigma_d^2 \mathbb{E}[T] + \mu_d^2\mathbb{E}[T^2]
		\\
		& =  \sigma_d^2 \mu_\ell + \mu_d^2\left(Var(T) + (\mathbb{E}[T])^2\right)
		\\
		& = \sigma_d^2 \mu_\ell + \mu_d^2\left(\sigma^2_\ell+ \mu_\ell^2\right).
	\end{align*}
	Finally,
	\begin{align*}
		Var(D(T)) &= \mathbb{E}[D^2(T)] - \big(\mathbb{E}[D(T)]\big)^2
		\\
		& =  \sigma_d^2 \mu_\ell + \mu_d^2\left(\sigma^2_\ell+ \mu_\ell^2\right) - (\mu_d \mu_\ell)^2
		\\
		& = \sigma_d^2 \mu_\ell + \mu_d^2 \sigma^2_\ell.
	\end{align*}
\end{proof}
\clearpage



\end{document}
